{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP-Website-Legitimation"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project is about to demonstrate how to use NLP to detect if a website is legitimate.\n",
    "Currently, work on English websites.\n",
    "\n",
    "Flow:\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step:1 - Libraries\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "#Data path folder\n",
    "path = 'G:/DataScienceProject/webcrawler/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On webcrawler part, we shell extract all href (links) into single html file.\n",
    "The idea is to collect all natual languages from all website part for further NLP catorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step:2 - WebCrawler\n",
    "def webcrawler(url, path):\n",
    "    print(\"Start webcrawler: \\n\", url )\n",
    "    def folderBuilder(url, path):\n",
    "        global newpath\n",
    "        global webpageName\n",
    "        position = [pos for pos, char in enumerate(url) if char == '.']\n",
    "        position[0] = int(position[0]) + 1\n",
    "        webpageName = url[position[0]:position[1]]\n",
    "\n",
    "        # Check / at end of path\n",
    "        len(path) - 1\n",
    "        if path[len(path) - 1:] == '/':\n",
    "            os.mkdir(path + webpageName)\n",
    "            newpath = path + webpageName\n",
    "        else:\n",
    "            os.mkdir(path + '/' + webpageName)\n",
    "            newpath = path + '/' + webpageName\n",
    "\n",
    "        print(\"Building website folder\")\n",
    "        return\n",
    "\n",
    "    def getData(url):\n",
    "        print(\"Get main url\")\n",
    "        global data\n",
    "        # Open the URL as Browser, not as python urllib\n",
    "        page = urllib.request.Request(url, headers={\n",
    "            'User-Agent': '\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.69 Safari/537.36\"'})\n",
    "        soup = urllib.request.urlopen(page).read()\n",
    "        data = BeautifulSoup(soup, \"html.parser\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def hrefAgg(data):\n",
    "        print(\"Create HREF list\")\n",
    "        global hrefStartList\n",
    "        # Aggregate href into file\n",
    "        hrefStartList = list(data.findAll('a'))\n",
    "\n",
    "        for i in range(len(hrefStartList)):\n",
    "            link = str(hrefStartList[i])\n",
    "            link = link[9:]\n",
    "            link = link[:link.find('\"')]\n",
    "            # Check for http/https\n",
    "            if link[0:4] == 'http':\n",
    "                hrefStartList[i] = link\n",
    "            else:\n",
    "                hrefStartList[i] = ''\n",
    "\n",
    "        # Remove dup\n",
    "        hrefStartList = list(dict.fromkeys(hrefStartList))\n",
    "        if hrefStartList.count('') == 1:\n",
    "            del hrefStartList[hrefStartList.index('')]\n",
    "\n",
    "        if hrefStartList != ['']:\n",
    "            href2file = newpath + '/' + 'href.txt'\n",
    "            file2write = open(href2file, 'w')\n",
    "            file2write.write(str(hrefStartList))\n",
    "            file2write.close()\n",
    "\n",
    "        return\n",
    "\n",
    "    def getMyHrefs(myUrl):\n",
    "        print(myUrl)\n",
    "        global data1\n",
    "        page = urllib.request.Request(url, headers={\n",
    "            'User-Agent': '\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.69 Safari/537.36\"'})\n",
    "        soup1 = urllib.request.urlopen(page).read()\n",
    "        data1 = BeautifulSoup(soup1, \"html.parser\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        return\n",
    "\n",
    "    folderBuilder(url, path)\n",
    "    getData(url)\n",
    "    hrefAgg(data)\n",
    "    data0 = str(data)\n",
    "    for i in range(len(hrefStartList)):\n",
    "        myUrl = hrefStartList[i]\n",
    "        getMyHrefs(myUrl)\n",
    "        data0 = data0 + str(data1)\n",
    "\n",
    "    AppendHref2File = newpath + '/' + 'full_website.txt'\n",
    "    file2write = open(AppendHref2File, 'w', encoding='utf8')\n",
    "    file2write.write(data0)\n",
    "    file2write.close()\n",
    "    print(\"Complete webcrawler\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start webcrawler: \n",
      " https://www.hbo.com/\n",
      "Building website folder\n",
      "Get main url\n",
      "Create HREF list\n",
      "https://www.hbo.com/insecure\n",
      "Complete webcrawler\n"
     ]
    }
   ],
   "source": [
    "#Step:3 - Run the webcrawler function\n",
    "#For websites list\n",
    "#Ural format: 'https://www.hbo.com/'\n",
    "testList = ['https://www.hbo.com/']\n",
    "for z, aurl in enumerate(testList):\n",
    "    try:\n",
    "        webcrawler(aurl, path)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step:4 - Delete empty folders\n",
    "import os\n",
    "\n",
    "dataFolderList = os.listdir(path)\n",
    "for a in dataFolderList:\n",
    "    folderDel = path + a\n",
    "    #Non empty floder will throw an error\n",
    "    try:\n",
    "        os.rmdir(folderDel)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "if os.path.exists(path) == False:\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step:5  - Libraries\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step:6 - html to text convertor function\n",
    "stemmer = WordNetLemmatizer()\n",
    "#Preprocess func\n",
    "def preprocess_text(document):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(document))\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    # Lemmatization\n",
    "    tokens = document.split()\n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in en_stop]\n",
    "    tokens = [word for word in tokens if len(word) > 3]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hbo\n"
     ]
    }
   ],
   "source": [
    "#Step:7 - run convertor\n",
    "for j, value in enumerate(dataFolderList):\n",
    "    #Load each site full_website.txt\n",
    "    fullWebFile = path + value + '/' + 'full_website.txt'\n",
    "    print(value)\n",
    "    try:\n",
    "        f = open(fullWebFile, 'r', encoding='utf-8')\n",
    "        data = f.read()\n",
    "        f.close()\n",
    "        data = BeautifulSoup(data, \"html.parser\")\n",
    "        text = data.get_text(strip=True)\n",
    "        #Break html into sentences\n",
    "        text = sent_tokenize(text)\n",
    "        text = preprocess_text(text)\n",
    "        myCleanFile = path + value + '/clean_' + value +'.txt'\n",
    "        file2write = open(myCleanFile, 'w', encoding='utf8')\n",
    "        file2write.write(text)\n",
    "        file2write.close()\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the category part, an early pre-phase in need as followed:\n",
    "- We need to create a dictionary by category.\n",
    "- I used the followed link: https://www.wordstream.com/popular-keywords\n",
    "- Dictionary example: {\"car\": \"1\", \"auto\": \"2\", \"used cars\": \"3\", \"autos\": \"4\", \"mechanic\": \"5\", \"automobile\": \"6\"}\n",
    "- File extension should be *.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step:8 - Handling the issue of common keywords in other categories & combine all dic into a single list (list of lists)\n",
    "import json\n",
    "dirpath = 'G:/DataScienceProject/webcrawler/'\n",
    "genCatDic = []\n",
    "results = []\n",
    "results += [each for each in os.listdir(dirpath) if each.endswith('.xlsx')]\n",
    "for elem in results:\n",
    "    listName = elem[:-5]\n",
    "    dicName = dirpath + elem\n",
    "    with open(dicName, 'r') as file:\n",
    "        dic = json.load(file)\n",
    "    genCatDic.append(list(dic))\n",
    "\n",
    "#Find non common keyword for category from each categogry type\n",
    "catDic = {}\n",
    "for i, value in enumerate(results):\n",
    "    commonList = []\n",
    "    for j in range(0,len(results)):\n",
    "        if j != i:\n",
    "            commonList += genCatDic[j]\n",
    "\n",
    "    diffList = list(set(genCatDic[i]).difference(commonList))\n",
    "    catDic[value[:-5]] = diffList\n",
    "\n",
    "filename = dirpath + 'categoriesDic.xlsx'\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(catDic, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step:9 - Category prediction\n",
    "import json\n",
    "\n",
    "dicName = 'G:/DataScienceProject/webcrawler/categoriesDic.xlsx'\n",
    "with open(dicName, 'r') as file:\n",
    "    dic = json.load(file)\n",
    "\n",
    "#Load target website after cleansing\n",
    "catList = os.listdir('G:/DataScienceProject/webcrawler/test')\n",
    "for d, valued in enumerate(catList):\n",
    "    filename = path + valued + '/clean_' +valued + '.txt'\n",
    "    f = open(filename, 'r', encoding='utf-8')\n",
    "    websiteData = f.read()\n",
    "    f.close()\n",
    "\n",
    "    #Category classifier\n",
    "    maxCatCounter = 0\n",
    "    for i, valueF in enumerate(dic.keys()):\n",
    "        currentCatCounter = 0\n",
    "        for j, valueG in enumerate(dic[valueF]):\n",
    "            if websiteData.count(valueG) > 0:\n",
    "                currentCatCounter += 1\n",
    "\n",
    "        if currentCatCounter > maxCatCounter:\n",
    "            maxCatCounter = currentCatCounter\n",
    "            catClass = valueF\n",
    "\n",
    "    filename = filename.replace(\"clean_\", \"\")\n",
    "    with open(filename, 'a+') as f:\n",
    "        f.write(f\"Followed report for: {valued}\\nWebsite category: {catClass}\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step:10 - Profanity check\n",
    "#Load Google profanity\n",
    "profanityPath = 'G:/DataScienceProject/webcrawler/Google-profanity.txt'\n",
    "with open(profanityPath, 'r') as f:\n",
    "    profanity = f.read()\n",
    "    f.close()\n",
    "\n",
    "profanity = profanity.split(\"\\n\")\n",
    "\n",
    "for e, valuee in enumerate(catList):\n",
    "    filename = path + valuee + '/' +valuee + '.txt'\n",
    "    #Load website\n",
    "    f = open(filename, 'r', encoding='utf-8')\n",
    "    websiteData = f.read()\n",
    "    f.close()\n",
    "    websiteData = websiteData.split(\" \")\n",
    "\n",
    "    if set(profanity).intersection(list(set(websiteData))) != 0:\n",
    "        msg = str(len(set(profanity).intersection(list(set(websiteData))))) + \" found\"\n",
    "    else:\n",
    "        msg = \"Clean\"\n",
    "\n",
    "    with open(filename, 'a+') as f:\n",
    "        f.write(f\"Profanity check: {msg}\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step:11 - SSL check for security function\n",
    "from urllib.request import ssl, socket\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def checkSSL(url, port):\n",
    "    try:\n",
    "        context = ssl.create_default_context()\n",
    "        with socket.create_connection((url, port), timeout=5) as sock:\n",
    "            with context.wrap_socket(sock, server_hostname=url) as ssock:\n",
    "                print(ssock.version())\n",
    "                data = json.dumps(ssock.getpeercert())\n",
    "    except:\n",
    "        msg = \"SSL chcek: connectivity issue\"\n",
    "\n",
    "    if data != '':\n",
    "        notBeforeCert = data[data.find(\"notBefore\")-1:].split('\"')[3]\n",
    "        notBeforeCert = notBeforeCert[:-4] #remove GMT\n",
    "        notAfterCert = data[data.find(\"notAfter\")-1:].split('\"')[3]\n",
    "        notAfterCert = notAfterCert[:-4]\n",
    "\n",
    "        today = datetime.date.today()\n",
    "        currentTime = time.mktime(today.timetuple())\n",
    "        timestruct1 = time.strptime(notBeforeCert, \"%b %d %H:%M:%S %Y\")\n",
    "        before = time.mktime(timestruct1)\n",
    "        timestruct2 = time.strptime(notAfterCert, \"%b %d %H:%M:%S %Y\")\n",
    "        after = time.mktime(timestruct2)\n",
    "\n",
    "        if before < currentTime < after:\n",
    "            msg = \"SSL chcek: valid\"\n",
    "        else:\n",
    "            msg = \"SSL chcek: certificate issue\"\n",
    "\n",
    "    file = url.split(\".\")[1]\n",
    "    path = 'G:/DataScienceProject/webcrawler/test/'\n",
    "    filename = path + file + '/' + file+ '.txt'\n",
    "    with open(filename, 'a+') as f:\n",
    "        f.write(f\"{msg}\\n\")\n",
    "        f.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLSv1.2\n"
     ]
    }
   ],
   "source": [
    "#Step:12 - Actual SSL check \n",
    "#url = 'www.walla.co.il'\n",
    "port = '443'\n",
    "#testList\n",
    "for i, value in enumerate(testList):\n",
    "    if value.count(\"/\") > 0:\n",
    "        value = value.split(\"/\")[2]\n",
    "        url = value\n",
    "    checkSSL(url, port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step:13 - Prepare for beb reviews\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "def websiteName(url):\n",
    "    global reviewUrl\n",
    "    global newpath\n",
    "    global webpageName\n",
    "    reviewUrl = url.split(\"/\")[2]\n",
    "    reviewUrl = reviewUrl.split(\"www.\")[1]\n",
    "    return reviewUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step:12 - Web review#1\n",
    "def getSitejabber(reviewUrl):\n",
    "    global data\n",
    "    reviewSiteUrl = 'https://www.sitejabber.com/reviews/' + reviewUrl\n",
    "    # Open the URL as Browser, not as python urllib\n",
    "    page = urllib.request.Request(reviewSiteUrl, headers={\n",
    "        'User-Agent': '\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.69 Safari/537.36\"'})\n",
    "    soup = urllib.request.urlopen(page).read()\n",
    "    data = BeautifulSoup(soup, \"html.parser\")\n",
    "    text = data.get_text(strip=True)\n",
    "    position = text.find('stars') #www.sitejabber.com\n",
    "    if position != -1:\n",
    "        text = text[position-5:position-1]\n",
    "    else:\n",
    "        text = 0\n",
    "\n",
    "    path = 'G:/DataScienceProject/webcrawler/test/'\n",
    "    fullFileName = path + reviewUrl[:-4] +  '/' + reviewUrl[:-4] +'.txt'\n",
    "    with open(fullFileName, 'a+') as f:\n",
    "        f.write(f\"website review#1: {text}\\n\")\n",
    "        f.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step:13 - Web review#2\n",
    "def getTrustpilot(reviewUrl):\n",
    "    global data\n",
    "    global js\n",
    "    reviewSiteUrl = 'https://www.trustpilot.com/review/www.' + reviewUrl\n",
    "    # Open the URL as Browser, not as python urllib\n",
    "    page = urllib.request.Request(reviewSiteUrl, headers={\n",
    "        'User-Agent': '\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.69 Safari/537.36\"'})\n",
    "    soup = urllib.request.urlopen(page).read()\n",
    "    data = BeautifulSoup(soup, \"html.parser\")\n",
    "    js=str(data.findAll('script'))\n",
    "    js = js[js.find('trustScore'): js.find('trustScore')+18].split(\",\")[0]\n",
    "    if js != '':\n",
    "        js = float(js.split(\":\")[1])\n",
    "    else:\n",
    "        js = 0\n",
    "\n",
    "    path = 'G:/DataScienceProject/webcrawler/test/'\n",
    "    fullFileName = path + reviewUrl[:-4] +  '/' + reviewUrl[:-4] +'.txt'\n",
    "    with open(fullFileName, 'a+') as f:\n",
    "        f.write(f\"website review#2: {js}\\n\")\n",
    "        f.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step:14 - actual run for web reviews\n",
    "for i, value in  enumerate(testList):\n",
    "    websiteName(value)\n",
    "    getSitejabber(reviewUrl)\n",
    "    getTrustpilot(reviewUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Followed report for: hbo\n",
      "Website category: Cellular\n",
      "Profanity check: 0 found\n",
      "SSL chcek: valid\n",
      "website review#1: 4.23\n",
      "website review#2: 1.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step:15 - Log view (jsu for demo)\n",
    "path = 'G:/DataScienceProject/webcrawler/test/'\n",
    "fullFileName = path + reviewUrl[:-4] +  '/' + reviewUrl[:-4] +'.txt'\n",
    "fullFileName\n",
    "with open(fullFileName, 'r') as f:\n",
    "        log=f.read()\n",
    "        f.close()\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclution\n",
    "\n",
    "Its the 1st NLP project from end to end & it's awesome!!!\n",
    "Further to this part, I'd like to check the image recognition for webstie legitimation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
